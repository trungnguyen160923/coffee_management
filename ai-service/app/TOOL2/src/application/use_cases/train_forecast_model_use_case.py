"""
Train Forecast Model Use Case
"""
from typing import Dict, Any
from datetime import date

from ...domain.entities.metrics import DailyBranchMetrics
from ...domain.repositories.metrics_repository import IMetricsRepository
from ...domain.repositories.model_repository import IModelRepository
from ...infrastructure.ml.forecast_trainer import ForecastTrainer


class TrainForecastModelUseCase:
    """Use case ƒë·ªÉ train forecasting model"""
    
    def __init__(self, metrics_repository: IMetricsRepository,
                 model_repository: IModelRepository):
        self.metrics_repository = metrics_repository
        self.model_repository = model_repository
        self.trainer = ForecastTrainer()
    
    def execute(self, branch_id: int,
                algorithm: str,
                target_metric: str,
                training_days: int = 90,
                rolling_window: int = None,
                model_version: str = "v1.0",
                created_by: str = "system",
                save_to_db: bool = True,
                **hyperparameters) -> Dict[str, Any]:
        """
        Train forecasting model
        
        Args:
            branch_id: ID chi nh√°nh
            algorithm: 'PROPHET', 'LIGHTGBM', ho·∫∑c 'XGBOOST'
            target_metric: Metric c·∫ßn d·ª± b√°o
            training_days: S·ªë ng√†y d·ªØ li·ªáu training
            rolling_window: S·ªë ng√†y rolling window (ch·ªâ train tr√™n N ng√†y cu·ªëi, None = d√πng to√†n b·ªô)
            model_version: Phi√™n b·∫£n model
            created_by: Ng∆∞·ªùi t·∫°o model
            **hyperparameters: Hyperparameters cho t·ª´ng algorithm
        
        Returns:
            Dict ch·ª©a model_id v√† metadata
        """
        # L·∫•y d·ªØ li·ªáu training
        metrics_list = self.metrics_repository.find_for_training(branch_id, days=training_days)
        
        if len(metrics_list) < 30:
            raise ValueError(f"C·∫ßn √≠t nh·∫•t 30 ng√†y d·ªØ li·ªáu. Hi·ªán c√≥: {len(metrics_list)} ng√†y")
        
        # √Åp d·ª•ng rolling window n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if rolling_window and rolling_window > 0:
            if rolling_window >= len(metrics_list):
                print(f"‚ö†Ô∏è  Rolling window ({rolling_window}) >= t·ªïng s·ªë samples ({len(metrics_list)}), d√πng to√†n b·ªô")
            else:
                # Ch·ªâ l·∫•y N ng√†y cu·ªëi
                metrics_list = metrics_list[-rolling_window:]
                print(f"üìä √Åp d·ª•ng rolling window: ch·ªâ train tr√™n {len(metrics_list)} ng√†y cu·ªëi")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu (c·∫ßn cho ƒë√°nh gi√°)
        df = self.trainer.prepare_time_series_data(metrics_list, target_metric)
        
        # Train model
        model, metadata = self.trainer.train(
            metrics_list,
            algorithm=algorithm,
            target_metric=target_metric,
            **hyperparameters
        )
        
        # L∆∞u metadata target_metric v√† rolling_window v√†o metadata
        metadata['target_metric'] = target_metric
        if rolling_window:
            metadata['rolling_window'] = rolling_window
            metadata['rolling_window_applied'] = True
        
        # ƒê√°nh gi√° model (train/test split)
        evaluation_metrics = None
        try:
            evaluation_metrics = self.trainer.evaluate_model(
                model=model,
                metadata=metadata,
                training_df=df,
                algorithm=algorithm,
                target_metric=target_metric,
                test_ratio=0.2  # 20% d·ªØ li·ªáu test
            )
            # L∆∞u evaluation metrics v√†o metadata
            metadata['evaluation_metrics'] = evaluation_metrics
        except Exception as e:
            # N·∫øu ƒë√°nh gi√° th·∫•t b·∫°i, v·∫´n ti·∫øp t·ª•c l∆∞u model
            metadata['evaluation_error'] = str(e)
        
        # L∆∞u model v√†o repository (n·∫øu save_to_db = True)
        model_id = None
        if save_to_db:
            model_id = self.trainer.save_model_to_repository(
                self.model_repository,
                branch_id,
                model,
                metadata,
                target_metric,
                algorithm,
                model_version,
                created_by
            )
        
        return {
            'model_id': model_id,
            'algorithm': algorithm,
            'target_metric': target_metric,
            'training_samples': len(metrics_list),
            'metadata': metadata,
            'evaluation_metrics': evaluation_metrics
        }

